{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP9AFlN0+8fYTqyP8r3Kqsu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ybw9000/torch_Dtensor_playground/blob/main/torch_Dtensor.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {
        "id": "6W0402TPI630"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.distributed.tensor import distribute_tensor, DTensor\n",
        "from torch.distributed.device_mesh import DeviceMesh\n",
        "from torch.distributed.tensor.placement_types import Shard, Replicate, Partial\n",
        "import torch.distributed.tensor as dtensor"
      ],
      "metadata": {
        "id": "-xrIpOBoJGTE"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.multiprocessing as mp\n",
        "import os"
      ],
      "metadata": {
        "id": "nx2QbtO7VqdX"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def setup_device(rank, world_size):\n",
        "    # need these otherwise it raises errors during process init\n",
        "    os.environ[\"MASTER_ADDR\"] = \"localhost\"\n",
        "    os.environ[\"MASTER_PORT\"] = \"56492\"\n",
        "    os.environ[\"RANK\"] = str(rank)  # Set the rank for each process\n",
        "    os.environ[\"WORLD_SIZE\"] = str(world_size)  # Set the world size\n",
        "\n",
        "def kshard_gemm(rank, world_size):\n",
        "    setup_device(rank, world_size)\n",
        "    with DeviceMesh('cpu', [0, 1]):\n",
        "        x = dtensor.ones((2, 2), placements=(Shard(1), ))\n",
        "        y = dtensor.ones((2, 2), placements=(Shard(0), ))\n",
        "        z = x.matmul(y)\n",
        "        print(z.placements)  # (Partial(sum),)\n",
        "        print(z.to_local())  # [1., 1.]\n",
        "        # need a manual replicate to indicate an all-reduce\n",
        "        z_ = z.redistribute(placements=(Replicate(), ))\n",
        "        print(z_.placements)\n",
        "        print(z_.to_local())  # [2., 2.]"
      ],
      "metadata": {
        "id": "YTPs7rwhKTEh"
      },
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "world_size = 2\n",
        "mp.start_processes(kshard_gemm, args=(world_size, ), nprocs=world_size, join=True, start_method='fork')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hFpoy1j2JHjR",
        "outputId": "1522c7d3-4c50-4127-d195-778f9b60964b"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(Partial(sum),)\n",
            "tensor([[1., 1.],\n",
            "        [1., 1.]])\n",
            "(Partial(sum),)\n",
            "tensor([[1., 1.],\n",
            "        [1., 1.]])\n",
            "(Replicate(),)(Replicate(),)\n",
            "\n",
            "tensor([[2., 2.],\n",
            "        [2., 2.]])tensor([[2., 2.],\n",
            "        [2., 2.]])\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def ffn(weight0, weight1, x, ln_weight, ln_bias):\n",
        "    out = x.matmul(weight0).relu().matmul(weight1)\n",
        "    return F.layer_norm(out, x.shape[1:], ln_weight, ln_bias, 1e-5)"
      ],
      "metadata": {
        "id": "Tu96y_8hWPA7"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.ones((2, 2))\n",
        "weight0 = torch.ones((2, 2))\n",
        "weight1 = torch.ones((2, 2))\n",
        "ln_weight = torch.ones(x.shape[1:]) / 2\n",
        "ln_bias = torch.ones(x.shape[1:]) + 0.5"
      ],
      "metadata": {
        "id": "5WZ11R4kbpdl"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ffn(weight0, weight1, x, ln_weight, ln_bias)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5wtEJkOQb9mx",
        "outputId": "0acf5c00-9ebf-4b11-cd63-c978dfdc86b7"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1.5000, 1.5000],\n",
              "        [1.5000, 1.5000]])"
            ]
          },
          "metadata": {},
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def sharded_ffn(rank, world_size, weight0, weight1, x, ln_weight, ln_bias):\n",
        "    setup_device(rank, world_size)\n",
        "    with DeviceMesh('cpu', [0, 1]):\n",
        "        # manual SPMD partition if using from_local api\n",
        "        # Dweight0 = DTensor.from_local(weight0.reshape(weight0.shape[0], 2, -1)[:, rank, :], placements=(Shard(1), ))\n",
        "        # Dweight1 = DTensor.from_local(weight1.reshape(2, -1, weight1.shape[-1])[rank, :, :], placements=(Shard(0), ))\n",
        "        # auto SPMD parition if using distribute_tensor api, NOTE: distribute_tensor only works with leaf tensors, aka not activations\n",
        "        Dweight0 = dtensor.distribute_tensor(weight0, placements=(Shard(1), ))\n",
        "        Dweight1 = dtensor.distribute_tensor(weight1, placements=(Shard(0), ))\n",
        "        print(Dweight0)\n",
        "        print(Dweight1)\n",
        "        Dx = DTensor.from_local(x, placements=(Replicate(), ))\n",
        "        Dln_weight = DTensor.from_local(ln_weight, placements=(Replicate(), ))\n",
        "        Dln_bias = DTensor.from_local(ln_bias, placements=(Replicate(), ))\n",
        "        Dout = ffn(Dweight0, Dweight1, Dx, Dln_weight, Dln_bias)\n",
        "        print(Dout.placements)\n",
        "        # Dout = Dout.redistribute(placements=(Replicate(), ))\n",
        "        print(Dout.to_local())"
      ],
      "metadata": {
        "id": "zgWljG8hcCJG"
      },
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mp.start_processes(sharded_ffn, args=(world_size, weight0, weight1, x, ln_weight, ln_bias), nprocs=world_size, join=True, start_method='fork')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NHXjvR-GcrZa",
        "outputId": "8dc3f7f3-2cd0-4969-9474-13bbf9e57c11"
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/distributed/tensor/_random.py:45: UserWarning: DTensor random operators may not have complete support on cpu device mesh\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/distributed/tensor/_random.py:45: UserWarning: DTensor random operators may not have complete support on cpu device mesh\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/distributed/tensor/_random.py:45: UserWarning: DTensor random operators may not have complete support on cpu device mesh\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/distributed/tensor/_random.py:45: UserWarning: DTensor random operators may not have complete support on cpu device mesh\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DTensor(local_tensor=tensor([[1.],\n",
            "        [1.]]), device_mesh=DeviceMesh('cpu', [0, 1]), placements=(Shard(dim=1),))DTensor(local_tensor=tensor([[1.],\n",
            "        [1.]]), device_mesh=DeviceMesh('cpu', [0, 1]), placements=(Shard(dim=1),))\n",
            "\n",
            "DTensor(local_tensor=tensor([[1., 1.]]), device_mesh=DeviceMesh('cpu', [0, 1]), placements=(Shard(dim=0),))DTensor(local_tensor=tensor([[1., 1.]]), device_mesh=DeviceMesh('cpu', [0, 1]), placements=(Shard(dim=0),))\n",
            "\n",
            "(Replicate(),)(Replicate(),)\n",
            "\n",
            "tensor([[1.5000, 1.5000],\n",
            "        [1.5000, 1.5000]])\n",
            "tensor([[1.5000, 1.5000],\n",
            "        [1.5000, 1.5000]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def ffn_no_ln(weight0, weight1, x):\n",
        "    out = x.matmul(weight0).relu().matmul(weight1)\n",
        "    return out\n",
        "\n",
        "def sharded_ffn_self_init(rank, world_size):\n",
        "    setup_device(rank, world_size)\n",
        "    with DeviceMesh('cpu', [0, 1]):\n",
        "        # random apis like randn does not work on cpus\n",
        "        Dx = dtensor.ones((2, 2), placements=(Replicate(), ))\n",
        "        Dweight0 = dtensor.ones((2, 2), placements=(Shard(1), ))\n",
        "        Dweight1 = dtensor.ones((2, 2), placements=(Shard(0), ))\n",
        "        Dout = ffn_no_ln(Dweight0, Dweight1, Dx)\n",
        "        Dout = Dout.redistribute(placements=(Replicate(), ))\n",
        "        print(Dout.to_local())"
      ],
      "metadata": {
        "id": "pmXiLC4sc3Zz"
      },
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mp.start_processes(sharded_ffn_self_init, args=(world_size, ), nprocs=world_size, join=True, start_method='fork')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TdP_h9dFeBa3",
        "outputId": "9b41cea5-d04c-41ee-f803-e678e3290152"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[4., 4.],\n",
            "        [4., 4.]])tensor([[4., 4.],\n",
            "        [4., 4.]])\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.randn((2, 2))\n",
        "weight0 = torch.randn((2, 2))\n",
        "weight1 = torch.randn((2, 2))\n",
        "ln_weight = torch.randn(x.shape[1:])\n",
        "ln_bias = torch.randn(x.shape[1:])"
      ],
      "metadata": {
        "id": "XLTwTMNHfi1C"
      },
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ffn(weight0, weight1, x, ln_weight, ln_bias)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PKfBxu-elxVH",
        "outputId": "df576dc6-74d4-4e6f-b59e-e745725d592d"
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1.4708, 0.5261],\n",
              "        [1.2591, 1.6434]])"
            ]
          },
          "metadata": {},
          "execution_count": 91
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mp.start_processes(sharded_ffn, args=(world_size, weight0, weight1, x, ln_weight, ln_bias), nprocs=world_size, join=True, start_method='fork')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fhtw5Z8Cl0lZ",
        "outputId": "e3a0cf97-b867-4ef6-96a9-9c7ab63fa1c5"
      },
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/distributed/tensor/_random.py:45: UserWarning: DTensor random operators may not have complete support on cpu device mesh\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/distributed/tensor/_random.py:45: UserWarning: DTensor random operators may not have complete support on cpu device mesh\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/distributed/tensor/_random.py:45: UserWarning: DTensor random operators may not have complete support on cpu device mesh\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/distributed/tensor/_random.py:45: UserWarning: DTensor random operators may not have complete support on cpu device mesh\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DTensor(local_tensor=tensor([[-0.8675],\n",
            "        [ 0.6228]]), device_mesh=DeviceMesh('cpu', [0, 1]), placements=(Shard(dim=1),))DTensor(local_tensor=tensor([[-1.4091],\n",
            "        [ 0.8076]]), device_mesh=DeviceMesh('cpu', [0, 1]), placements=(Shard(dim=1),))\n",
            "DTensor(local_tensor=tensor([[ 1.8077, -0.5340]]), device_mesh=DeviceMesh('cpu', [0, 1]), placements=(Shard(dim=0),))\n",
            "\n",
            "DTensor(local_tensor=tensor([[-0.2621,  0.6153]]), device_mesh=DeviceMesh('cpu', [0, 1]), placements=(Shard(dim=0),))\n",
            "(Replicate(),)(Replicate(),)\n",
            "tensor([[1.4708, 0.5261],\n",
            "        [1.2591, 1.6434]])\n",
            "\n",
            "tensor([[1.4708, 0.5261],\n",
            "        [1.2591, 1.6434]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MLP(nn.Module):\n",
        "\n",
        "    def __init__(self, in_dim, hidden_dim, out_dim):\n",
        "        super().__init__()\n",
        "        self.up_gemm = nn.Linear(in_dim, hidden_dim)\n",
        "        self.down_gemm = nn.Linear(hidden_dim, out_dim)\n",
        "        self.ln = nn.LayerNorm(in_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.up_gemm(x)\n",
        "        out = F.relu(out)\n",
        "        out = self.down_gemm(out)\n",
        "        # should be a prenorm yet we put it here to auto propagate an all-reduce\n",
        "        out = self.ln(out)\n",
        "        return out + x\n",
        "\n",
        "    def to_dtensor(self):\n",
        "        self.up_gemm.weight = nn.Parameter(dtensor.distribute_tensor(self.up_gemm.weight, placements=(Shard(1), )))\n",
        "        # auto complete by gemini below\n",
        "        self.up_gemm.bias = nn.Parameter(dtensor.distribute_tensor(self.up_gemm.bias, placements=(Replicate(), )))\n",
        "        self.down_gemm.weight = nn.Parameter(dtensor.distribute_tensor(self.down_gemm.weight, placements=(Shard(0), )))\n",
        "        self.down_gemm.bias = nn.Parameter(dtensor.distribute_tensor(self.down_gemm.bias, placements=(Replicate(), )))\n",
        "        self.ln.weight = nn.Parameter(dtensor.distribute_tensor(self.ln.weight, placements=(Replicate(), )))\n",
        "        self.ln.bias = nn.Parameter(dtensor.distribute_tensor(self.ln.bias, placements=(Replicate(), )))"
      ],
      "metadata": {
        "id": "fNuqhtsDl3H1"
      },
      "execution_count": 105,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sharded_mlp(rank, world_size, model, x):\n",
        "    setup_device(rank, world_size)\n",
        "    with DeviceMesh('cpu', [0, 1]):\n",
        "        x = dtensor.distribute_tensor(x, placements=(Replicate(), ))\n",
        "        model.to_dtensor()\n",
        "        out = model(x)\n",
        "        print(out.to_local())\n"
      ],
      "metadata": {
        "id": "fxEakAQHnqOH"
      },
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.randn((2, 128))\n",
        "mlp = MLP(128, 512, 128)"
      ],
      "metadata": {
        "id": "Qx44Am9lpD-e"
      },
      "execution_count": 102,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mlp(x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eg9r4UJJpNPn",
        "outputId": "30b1db55-94e7-459f-90d0-d6ea28a6a949"
      },
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-1.4708,  0.1171,  1.7956, -2.0253, -3.2707, -0.2634, -1.3897,  0.5627,\n",
              "          1.4990,  1.8911, -0.7884, -2.0117, -0.8167, -0.1332, -1.1860, -1.1104,\n",
              "         -0.7379, -1.3655, -0.7776, -1.9322, -0.5902,  1.7879,  1.6158,  0.4063,\n",
              "         -1.5333,  1.5448, -3.2080, -0.9128,  0.2484, -1.6977,  1.2191, -0.8326,\n",
              "          0.6986,  0.3736,  1.4546,  0.0133, -1.0566, -0.8257,  0.7642,  0.2890,\n",
              "         -1.7559, -0.0089,  0.1052,  0.4554,  1.7395, -2.4393,  0.8127, -0.7082,\n",
              "          0.7158,  3.0469, -3.6085, -0.6601,  1.3015,  1.9104, -1.6649,  1.1639,\n",
              "         -2.9202, -0.6016, -0.6613, -3.0803, -1.4661,  1.8791,  0.3352,  1.0842,\n",
              "          1.1850, -0.6935,  0.2724, -2.2499, -0.4073,  0.2196, -0.0576, -1.8649,\n",
              "          0.3465,  0.0938,  2.5835,  0.6649, -1.1765, -1.2938,  0.7223,  1.2641,\n",
              "         -0.0663,  3.2695,  1.0942,  0.5473,  1.6449, -0.3207, -0.3700,  0.3826,\n",
              "         -1.0532, -0.4288,  1.0137, -1.0178,  0.5352,  3.4009,  0.1376,  1.1851,\n",
              "         -1.0735,  1.6331, -1.1107, -2.3470,  0.2462,  2.3268,  2.9934,  0.1194,\n",
              "          0.6432,  1.4939,  0.8404, -2.6012,  0.6979, -2.4523, -1.1510,  0.0858,\n",
              "          0.7619, -0.8784,  0.9458,  0.2988, -0.2593,  0.2773,  0.5365, -0.0325,\n",
              "         -0.9290,  0.5514, -0.8726,  1.1467, -0.1017, -0.7618,  0.0423, -0.1121],\n",
              "        [ 1.5290, -0.7653, -0.5656,  0.9634, -3.4059, -0.8258, -1.4902, -0.1633,\n",
              "          1.8531, -0.1802,  0.2353, -0.9897,  1.9578,  0.7128,  1.3963, -1.0077,\n",
              "         -2.0416,  0.9264, -1.1562, -0.7625, -2.1485, -1.2724,  1.1895,  1.0822,\n",
              "          0.1085, -0.7096, -0.6092, -0.7145,  1.4451, -3.4005,  1.8390,  1.4345,\n",
              "         -0.2326, -1.5618,  0.7704,  2.9179, -0.8495, -0.6146, -1.4170,  2.8748,\n",
              "          1.7237, -1.1664, -0.6605,  1.4935,  1.1610, -0.5627,  0.0940, -1.6596,\n",
              "          3.0621, -1.4536,  0.5638,  1.7331, -1.0663,  0.9546,  0.7890, -1.1192,\n",
              "          0.6966,  1.0157,  0.6046, -1.9895,  0.3400, -0.1893, -1.0022, -1.4948,\n",
              "         -1.6266,  0.4160, -0.2970,  0.5020, -0.8502, -0.1738,  3.5439,  1.7852,\n",
              "          0.4787,  3.0237, -0.0630, -0.7242, -1.0613,  0.5684, -0.0136, -0.5421,\n",
              "          3.9244, -2.0661,  0.2625, -0.8757,  0.3169, -0.5260, -1.2317,  0.9802,\n",
              "         -1.5360, -0.6635, -0.0684, -0.8474,  1.9849, -0.3984,  1.4879, -0.1845,\n",
              "         -0.3113, -1.4215,  1.4683, -0.1520, -1.7087,  1.2019,  1.6002,  0.9916,\n",
              "          1.4968, -2.0274,  3.2035, -0.2578,  0.1039, -1.6389,  1.7550,  0.5163,\n",
              "          1.2348, -2.1507, -1.0776,  0.7999, -1.2701, -0.5281,  1.4177, -3.0477,\n",
              "         -0.4525,  5.8341, -1.6283,  0.1970,  0.1754, -1.5577, -2.0469,  2.2405]],\n",
              "       grad_fn=<AddBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 103
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mp.start_processes(sharded_mlp, args=(world_size, mlp, x), nprocs=world_size, join=True, start_method='fork')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OfwveYQ3pOEM",
        "outputId": "707ca934-511a-4efa-f37e-34c82bb8e088"
      },
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/distributed/tensor/_random.py:45: UserWarning: DTensor random operators may not have complete support on cpu device mesh\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/distributed/tensor/_random.py:45: UserWarning: DTensor random operators may not have complete support on cpu device mesh\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/distributed/tensor/_random.py:45: UserWarning: DTensor random operators may not have complete support on cpu device mesh\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/distributed/tensor/_random.py:45: UserWarning: DTensor random operators may not have complete support on cpu device mesh\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[-1.4708,  0.1171,  1.7956, -2.0253, -3.2707, -0.2634, -1.3897,  0.5627,\n",
            "          1.4990,  1.8911, -0.7884, -2.0117, -0.8167, -0.1332, -1.1860, -1.1104,\n",
            "         -0.7379, -1.3655, -0.7776, -1.9322, -0.5902,  1.7879,  1.6158,  0.4063,\n",
            "         -1.5333,  1.5448, -3.2080, -0.9128,  0.2484, -1.6977,  1.2191, -0.8326,\n",
            "          0.6986,  0.3736,  1.4546,  0.0133, -1.0566, -0.8257,  0.7642,  0.2890,\n",
            "         -1.7559, -0.0089,  0.1052,  0.4554,  1.7395, -2.4393,  0.8127, -0.7082,\n",
            "          0.7158,  3.0469, -3.6085, -0.6601,  1.3015,  1.9104, -1.6649,  1.1639,\n",
            "         -2.9202, -0.6016, -0.6613, -3.0803, -1.4661,  1.8791,  0.3352,  1.0842,\n",
            "          1.1850, -0.6935,  0.2724, -2.2499, -0.4073,  0.2196, -0.0576, -1.8649,\n",
            "          0.3465,  0.0938,  2.5835,  0.6649, -1.1765, -1.2938,  0.7223,  1.2641,\n",
            "         -0.0663,  3.2695,  1.0942,  0.5473,  1.6449, -0.3207, -0.3700,  0.3826,\n",
            "         -1.0532, -0.4288,  1.0137, -1.0178,  0.5352,  3.4009,  0.1376,  1.1851,\n",
            "         -1.0735,  1.6331, -1.1107, -2.3470,  0.2462,  2.3268,  2.9934,  0.1194,\n",
            "          0.6432,  1.4939,  0.8404, -2.6012,  0.6979, -2.4523, -1.1510,  0.0858,\n",
            "          0.7619, -0.8784,  0.9458,  0.2988, -0.2593,  0.2773,  0.5365, -0.0325,\n",
            "         -0.9290,  0.5514, -0.8726,  1.1467, -0.1017, -0.7618,  0.0423, -0.1121],\n",
            "        [ 1.5290, -0.7653, -0.5656,  0.9634, -3.4059, -0.8258, -1.4902, -0.1633,\n",
            "          1.8531, -0.1802,  0.2353, -0.9897,  1.9578,  0.7128,  1.3963, -1.0077,\n",
            "         -2.0416,  0.9264, -1.1562, -0.7625, -2.1485, -1.2724,  1.1895,  1.0822,\n",
            "          0.1085, -0.7096, -0.6092, -0.7145,  1.4451, -3.4005,  1.8390,  1.4345,\n",
            "         -0.2326, -1.5618,  0.7704,  2.9179, -0.8495, -0.6146, -1.4170,  2.8748,\n",
            "          1.7237, -1.1664, -0.6605,  1.4935,  1.1610, -0.5627,  0.0940, -1.6596,\n",
            "          3.0621, -1.4536,  0.5638,  1.7331, -1.0663,  0.9546,  0.7890, -1.1192,\n",
            "          0.6966,  1.0157,  0.6046, -1.9895,  0.3400, -0.1893, -1.0022, -1.4948,\n",
            "         -1.6266,  0.4160, -0.2970,  0.5020, -0.8502, -0.1738,  3.5439,  1.7852,\n",
            "          0.4787,  3.0237, -0.0630, -0.7242, -1.0613,  0.5684, -0.0136, -0.5421,\n",
            "          3.9244, -2.0661,  0.2625, -0.8757,  0.3169, -0.5260, -1.2317,  0.9802,\n",
            "         -1.5360, -0.6635, -0.0684, -0.8474,  1.9849, -0.3984,  1.4879, -0.1845,\n",
            "         -0.3113, -1.4215,  1.4683, -0.1520, -1.7087,  1.2019,  1.6002,  0.9916,\n",
            "          1.4968, -2.0274,  3.2035, -0.2578,  0.1039, -1.6389,  1.7550,  0.5163,\n",
            "          1.2348, -2.1507, -1.0776,  0.7999, -1.2701, -0.5281,  1.4177, -3.0477,\n",
            "         -0.4525,  5.8341, -1.6283,  0.1970,  0.1754, -1.5577, -2.0469,  2.2405]],\n",
            "       grad_fn=<_ToTorchTensorBackward>)\n",
            "tensor([[-1.4708,  0.1171,  1.7956, -2.0253, -3.2707, -0.2634, -1.3897,  0.5627,\n",
            "          1.4990,  1.8911, -0.7884, -2.0117, -0.8167, -0.1332, -1.1860, -1.1104,\n",
            "         -0.7379, -1.3655, -0.7776, -1.9322, -0.5902,  1.7879,  1.6158,  0.4063,\n",
            "         -1.5333,  1.5448, -3.2080, -0.9128,  0.2484, -1.6977,  1.2191, -0.8326,\n",
            "          0.6986,  0.3736,  1.4546,  0.0133, -1.0566, -0.8257,  0.7642,  0.2890,\n",
            "         -1.7559, -0.0089,  0.1052,  0.4554,  1.7395, -2.4393,  0.8127, -0.7082,\n",
            "          0.7158,  3.0469, -3.6085, -0.6601,  1.3015,  1.9104, -1.6649,  1.1639,\n",
            "         -2.9202, -0.6016, -0.6613, -3.0803, -1.4661,  1.8791,  0.3352,  1.0842,\n",
            "          1.1850, -0.6935,  0.2724, -2.2499, -0.4073,  0.2196, -0.0576, -1.8649,\n",
            "          0.3465,  0.0938,  2.5835,  0.6649, -1.1765, -1.2938,  0.7223,  1.2641,\n",
            "         -0.0663,  3.2695,  1.0942,  0.5473,  1.6449, -0.3207, -0.3700,  0.3826,\n",
            "         -1.0532, -0.4288,  1.0137, -1.0178,  0.5352,  3.4009,  0.1376,  1.1851,\n",
            "         -1.0735,  1.6331, -1.1107, -2.3470,  0.2462,  2.3268,  2.9934,  0.1194,\n",
            "          0.6432,  1.4939,  0.8404, -2.6012,  0.6979, -2.4523, -1.1510,  0.0858,\n",
            "          0.7619, -0.8784,  0.9458,  0.2988, -0.2593,  0.2773,  0.5365, -0.0325,\n",
            "         -0.9290,  0.5514, -0.8726,  1.1467, -0.1017, -0.7618,  0.0423, -0.1121],\n",
            "        [ 1.5290, -0.7653, -0.5656,  0.9634, -3.4059, -0.8258, -1.4902, -0.1633,\n",
            "          1.8531, -0.1802,  0.2353, -0.9897,  1.9578,  0.7128,  1.3963, -1.0077,\n",
            "         -2.0416,  0.9264, -1.1562, -0.7625, -2.1485, -1.2724,  1.1895,  1.0822,\n",
            "          0.1085, -0.7096, -0.6092, -0.7145,  1.4451, -3.4005,  1.8390,  1.4345,\n",
            "         -0.2326, -1.5618,  0.7704,  2.9179, -0.8495, -0.6146, -1.4170,  2.8748,\n",
            "          1.7237, -1.1664, -0.6605,  1.4935,  1.1610, -0.5627,  0.0940, -1.6596,\n",
            "          3.0621, -1.4536,  0.5638,  1.7331, -1.0663,  0.9546,  0.7890, -1.1192,\n",
            "          0.6966,  1.0157,  0.6046, -1.9895,  0.3400, -0.1893, -1.0022, -1.4948,\n",
            "         -1.6266,  0.4160, -0.2970,  0.5020, -0.8502, -0.1738,  3.5439,  1.7852,\n",
            "          0.4787,  3.0237, -0.0630, -0.7242, -1.0613,  0.5684, -0.0136, -0.5421,\n",
            "          3.9244, -2.0661,  0.2625, -0.8757,  0.3169, -0.5260, -1.2317,  0.9802,\n",
            "         -1.5360, -0.6635, -0.0684, -0.8474,  1.9849, -0.3984,  1.4879, -0.1845,\n",
            "         -0.3113, -1.4215,  1.4683, -0.1520, -1.7087,  1.2019,  1.6002,  0.9916,\n",
            "          1.4968, -2.0274,  3.2035, -0.2578,  0.1039, -1.6389,  1.7550,  0.5163,\n",
            "          1.2348, -2.1507, -1.0776,  0.7999, -1.2701, -0.5281,  1.4177, -3.0477,\n",
            "         -0.4525,  5.8341, -1.6283,  0.1970,  0.1754, -1.5577, -2.0469,  2.2405]],\n",
            "       grad_fn=<_ToTorchTensorBackward>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def my_custom_backend(gm: torch.fx.GraphModule, example_inputs):\n",
        "    for node in gm.graph.nodes:\n",
        "        print(node)\n",
        "    print(example_inputs)\n",
        "    return gm.forward\n",
        "\n",
        "@torch.compile(backend=my_custom_backend, )\n",
        "def ffn_compile(weight0, weight1, x):\n",
        "    out = x.matmul(weight0).relu().matmul(weight1)\n",
        "    return out\n",
        "\n",
        "def sharded_ffn_compile(rank, world_size):\n",
        "    setup_device(rank, world_size)\n",
        "    with DeviceMesh('cpu', [0, 1]):\n",
        "        # random apis like randn does not work on cpus\n",
        "        Dx = dtensor.ones((2, 2), placements=(Replicate(), ))\n",
        "        Dweight0 = dtensor.ones((2, 2), placements=(Shard(1), ))\n",
        "        Dweight1 = dtensor.ones((2, 2), placements=(Shard(0), ))\n",
        "        Dout = ffn_compile(Dweight0, Dweight1, Dx)\n",
        "        Dout = Dout.redistribute(placements=(Replicate(), ))\n",
        "        print(Dout.to_local())"
      ],
      "metadata": {
        "id": "KywR7fznpbyx"
      },
      "execution_count": 114,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mp.start_processes(sharded_ffn_compile, args=(world_size, ), nprocs=world_size, join=True, start_method='fork')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EPrhrDQlq-BP",
        "outputId": "79afc242-6abb-4754-f131-f0e7b2047aa6"
      },
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "l_x_\n",
            "l_weight0_l_x_\n",
            "\n",
            "l_weight0_\n",
            "l_weight1_l_weight1_\n",
            "\n",
            "matmul\n",
            "matmulrelu\n",
            "relu\n",
            "\n",
            "out\n",
            "outoutput\n",
            "\n",
            "output\n",
            "[DTensor(local_tensor=tensor([[1., 1.],\n",
            "        [1., 1.]]), device_mesh=DeviceMesh('cpu', [0, 1]), placements=(Replicate(),)), DTensor(local_tensor=tensor([[1.],\n",
            "        [1.]]), device_mesh=DeviceMesh('cpu', [0, 1]), placements=(Shard(dim=1),)), DTensor(local_tensor=tensor([[1., 1.]]), device_mesh=DeviceMesh('cpu', [0, 1]), placements=(Shard(dim=0),))]\n",
            "[DTensor(local_tensor=tensor([[1., 1.],\n",
            "        [1., 1.]]), device_mesh=DeviceMesh('cpu', [0, 1]), placements=(Replicate(),)), DTensor(local_tensor=tensor([[1.],\n",
            "        [1.]]), device_mesh=DeviceMesh('cpu', [0, 1]), placements=(Shard(dim=1),)), DTensor(local_tensor=tensor([[1., 1.]]), device_mesh=DeviceMesh('cpu', [0, 1]), placements=(Shard(dim=0),))]\n",
            "tensor([[4., 4.],\n",
            "        [4., 4.]])tensor([[4., 4.],\n",
            "        [4., 4.]])\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FilKnKbxrGKa"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}